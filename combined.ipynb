{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3bc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class CaptchaDatasetWithBBoxes(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Direct path to the specific data folder (train, val, or test)\n",
    "            transform (callable, optional): Optional transform to be applied on images\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.images_dir = os.path.join(self.data_dir, 'images')\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_list = sorted([f for f in os.listdir(self.images_dir) if f.endswith('.png')])\n",
    "        \n",
    "        # Load labels if available\n",
    "        self.labels_dict = {}\n",
    "        labels_file = os.path.join(self.data_dir, 'labels.json')\n",
    "        if os.path.exists(labels_file):\n",
    "            with open(labels_file, 'r') as f:\n",
    "                labels = json.load(f)\n",
    "                self.labels_dict = {item['image_id']: item for item in labels}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        \n",
    "        # Load image as grayscale\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        original_size = image.size  # (width, height)\n",
    "        \n",
    "        # Get image_id without extension\n",
    "        image_id = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        # Get labels if available\n",
    "        label_info = self.labels_dict.get(image_id, {})\n",
    "        \n",
    "        # Extract captcha string and annotations\n",
    "        captcha_string = label_info.get('captcha_string', '')\n",
    "        annotations = label_info.get('annotations', [])\n",
    "        \n",
    "        # Process bounding boxes\n",
    "        bboxes = []\n",
    "        oriented_bboxes = []\n",
    "        category_ids = []\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            # Regular bounding box [x1, y1, x2, y2]\n",
    "            bbox = annotation.get('bbox', [])\n",
    "            if bbox:\n",
    "                bboxes.append(bbox)\n",
    "            \n",
    "            # Oriented bounding box [x1,y1,x2,y2,x3,y3,x4,y4]\n",
    "            oriented_bbox = annotation.get('oriented_bbox', [])\n",
    "            if oriented_bbox:\n",
    "                oriented_bboxes.append(oriented_bbox)\n",
    "            \n",
    "            # Category ID (character class)\n",
    "            category_id = annotation.get('category_id', -1)\n",
    "            category_ids.append(category_id)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32) if bboxes else torch.empty((0, 4))\n",
    "        oriented_bboxes = torch.tensor(oriented_bboxes, dtype=torch.float32) if oriented_bboxes else torch.empty((0, 8))\n",
    "        category_ids = torch.tensor(category_ids, dtype=torch.long) if category_ids else torch.empty((0,), dtype=torch.long)\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'image_id': image_id,\n",
    "            'captcha_string': captcha_string,\n",
    "            'bboxes': bboxes,  # Regular bounding boxes\n",
    "            'oriented_bboxes': oriented_bboxes,  # Oriented bounding boxes\n",
    "            'category_ids': category_ids,  # Character category IDs\n",
    "            'num_objects': len(annotations),  # Number of characters\n",
    "            'original_size': original_size  # (width, height)\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# Custom collate function for bounding boxes\n",
    "def collate_fn_with_bboxes(batch):\n",
    "    \"\"\"Custom collate function to handle variable-length bounding boxes\"\"\"\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    image_ids = [item['image_id'] for item in batch]\n",
    "    captcha_strings = [item['captcha_string'] for item in batch]\n",
    "    \n",
    "    # Keep bounding boxes as lists since they have variable lengths\n",
    "    bboxes = [item['bboxes'] for item in batch]\n",
    "    oriented_bboxes = [item['oriented_bboxes'] for item in batch]\n",
    "    category_ids = [item['category_ids'] for item in batch]\n",
    "    num_objects = [item['num_objects'] for item in batch]\n",
    "    original_sizes = [item['original_size'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'image_id': image_ids,\n",
    "        'captcha_string': captcha_strings,\n",
    "        'bboxes': bboxes,\n",
    "        'oriented_bboxes': oriented_bboxes,\n",
    "        'category_ids': category_ids,\n",
    "        'num_objects': num_objects,\n",
    "        'original_size': original_sizes\n",
    "    }\n",
    "\n",
    "# Helper function to create dataloaders with bboxes\n",
    "def get_dataloader_with_bboxes(data_folder, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create dataloader that includes bounding box information\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = CaptchaDatasetWithBBoxes(\n",
    "        data_dir=data_folder,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create dataloader with custom collate function\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn_with_bboxes\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Visualization function with bounding boxes\n",
    "def visualize_with_bboxes(batch, idx=0):\n",
    "    \"\"\"Visualize image with bounding boxes\"\"\"\n",
    "    # Get image and denormalize\n",
    "    img = batch['image'][idx].squeeze()\n",
    "    img = img * 0.5 + 0.5  # Denormalize from [-1, 1] to [0, 1]\n",
    "    \n",
    "    # Get bounding boxes for this image\n",
    "    bboxes = batch['bboxes'][idx]\n",
    "    category_ids = batch['category_ids'][idx]\n",
    "    captcha_string = batch['captcha_string'][idx]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f\"CAPTCHA: {captcha_string}\")\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = plt.Rectangle((x1, y1), width, height, \n",
    "                           linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add category ID as text\n",
    "        if i < len(category_ids):\n",
    "            ax.text(x1, y1-5, f'ID: {category_ids[i].item()}', \n",
    "                   color='red', fontsize=10, weight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Number of characters: {len(bboxes)}\")\n",
    "    print(f\"Bounding boxes shape: {bboxes.shape}\")\n",
    "    print(f\"Category IDs: {category_ids.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3bbec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Image shape: torch.Size([4, 1, 160, 640])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 2\n",
      "Bounding boxes shape: torch.Size([2, 4])\n",
      "Category IDs: [2, 16]\n",
      "Image 0: '2G' has 2 characters\n",
      "  Bboxes shape: torch.Size([2, 4])\n",
      "Image 1: '346' has 3 characters\n",
      "  Bboxes shape: torch.Size([3, 4])\n",
      "Image 2: 'USB' has 3 characters\n",
      "  Bboxes shape: torch.Size([3, 4])\n",
      "Image 3: 'Z5A55S' has 6 characters\n",
      "  Bboxes shape: torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Base path to the dataset\n",
    "base_path = '/home/utn/abap44us/Downloads/UTN-CV25-Captcha-Dataset/part2'\n",
    "\n",
    "# Create dataloaders with bounding boxes\n",
    "train_loader_bbox = get_dataloader_with_bboxes(os.path.join(base_path, 'train'), batch_size=4, shuffle=True)\n",
    "\n",
    "# Test the dataloader\n",
    "for batch in train_loader_bbox:\n",
    "    print(f\"Batch size: {len(batch['captcha_string'])}\")\n",
    "    print(f\"Image shape: {batch['image'].shape}\")\n",
    "    \n",
    "    # Visualize first image with bounding boxes\n",
    "    visualize_with_bboxes(batch, idx=0)\n",
    "    \n",
    "    # Print info about bounding boxes\n",
    "    for i in range(len(batch['captcha_string'])):\n",
    "        print(f\"Image {i}: '{batch['captcha_string'][i]}' has {batch['num_objects'][i]} characters\")\n",
    "        print(f\"  Bboxes shape: {batch['bboxes'][i].shape}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d2ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 268800])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Weight Inititializeion with He-Method ) ---\n",
    "def init_weights_kaiming(m):\n",
    "    # use Kaiming init for conv/linear layers, set bias to zero\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    # batch/group norm: start with weight=1 and bias=0\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#  Stem \n",
    "class Stem(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a small-image friendly stem that mimics the res-net18 style stem.\n",
    "\n",
    "    Tradition ResNet stem:\n",
    "        # self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # self.bn   = nn.BatchNorm2d(out_ch)\n",
    "        # self.relu = nn.ReLU(inplace=True)\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    Compared to traditional. resnet 18 this  replace 7x7 s2 + maxpool with 3×(3x3) conv stack and delay the first downsample.\n",
    "    Goal: retain fineer details early in network\n",
    "    \"\"\"\n",
    "    # def __init__(self, in_ch=3, out_ch=64):  # RGB input channels\n",
    "    def __init__(self, in_ch=1, out_ch=64):    # Grayscale Input Channels\n",
    "        super().__init__()\n",
    "\n",
    "        # --- New small-image stem ---\n",
    "        self.conv1 = nn.Conv2d(in_ch, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.act1  = nn.SiLU(inplace=True)  # see https://arxiv.org/pdf/1710.05941\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.act2  = nn.SiLU(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, out_ch, kernel_size=3, stride=2, padding=1, bias=False)  # first downsample here\n",
    "        self.bn3   = nn.BatchNorm2d(out_ch)\n",
    "        self.act3  = nn.SiLU(inplace=True)\n",
    "\n",
    "        # Initialize weights of conv layers via He_Init Strategy\n",
    "        for m in [self.conv1, self.conv2, self.conv3]:\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        for b in [self.bn1, self.bn2, self.bn3]:\n",
    "            nn.init.constant_(b.weight, 1.0)\n",
    "            nn.init.constant_(b.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "        return x  # 160x640 -> 80x320 (H×W)\n",
    "\n",
    "#  Residual Block used for resnet\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_ch, out_ch, stride=1, dilation=1 , downsample=None):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        [IMPROVEMENT] allow dilation to grow RF without more downsamples\n",
    "        [IMPROVEMENT] SiLU activations for smoother gradients on OCR edges\n",
    "        \"\"\"\n",
    "        padding = dilation\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride,\n",
    "                               padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.act   = nn.SiLU(inplace=True)  # was ReLU\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1,\n",
    "                               padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.downsample = downsample   # <--- store it\n",
    "\n",
    "\n",
    "        # Init\n",
    "        for m in (self.conv1, self.conv2):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.bn1.weight, 1.0); nn.init.constant_(self.bn1.bias, 0.0)\n",
    "        nn.init.constant_(self.bn2.weight, 1.0); nn.init.constant_(self.bn2.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.downsample is not None:   # <--- apply projection if needed\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "# ---------- ResNet-18 Backbone ----------\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Backbone only (no GAP/FC).\n",
    "\n",
    "    Old vanilla scales (for 640x160 W×H) were deeper OS; we keep higher spatial detail:\n",
    "      stem  (s2) -> 80x320\n",
    "      layer1 (s1) -> 80x320\n",
    "      layer2 (s2) -> 40x160\n",
    "      layer3 (s1, dil=2) -> 40x160\n",
    "      layer4 (s1, dil=2) -> 40x160   (256 ch)  <-- feed to your head(s)\n",
    "\n",
    "    Goal: better localization of small characters while keeping receptive field via dilation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, return_p3=True):\n",
    "        super().__init__()\n",
    "        self.stem = Stem(in_ch=in_ch, out_ch=64)\n",
    "        self.in_ch = 64\n",
    "        self.return_p3 = return_p3\n",
    "\n",
    "        # layer1: stride=1\n",
    "        self.layer1 = self._make_layer(ResidualBlock, 64,  blocks=2, stride=1, dilation=1)\n",
    "\n",
    "        # layer2: stride=2 (downsample to ~1/4 overall)\n",
    "        self.layer2 = self._make_layer(ResidualBlock, 128, blocks=2, stride=2, dilation=1)\n",
    "\n",
    "        # layer3: keep resolution (stride=1) but expand RF with dilation\n",
    "        self.layer3 = self._make_layer(ResidualBlock, 256, blocks=2, stride=1, dilation=2)\n",
    "\n",
    "        # layer4: same resolution (stride=1) with dilation\n",
    "        self.layer4 = self._make_layer(ResidualBlock, 256, blocks=2, stride=1, dilation=2)  # slimmer tail (256)\n",
    "\n",
    "    def _make_layer(self, block, out_ch, blocks, stride, dilation):\n",
    "        down = None\n",
    "        in_ch = self.in_ch\n",
    "    \n",
    "        # Projection when spatial size or channels change\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "    \n",
    "        layers = [block(in_ch, out_ch, stride=stride, dilation=dilation, downsample=down)]\n",
    "        self.in_ch = out_ch\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_ch, out_ch, stride=1, dilation=dilation, downsample=None))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)         # -> 80x320\n",
    "        x = self.layer1(x)      # -> 80x320\n",
    "        x = self.layer2(x)     # -> 40x160\n",
    "        x = self.layer3(x)     # -> 40x160\n",
    "        x = self.layer4(x)     # -> 40x160\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dummy = torch.randn(1, 1, 160, 640)  # grayscale CAPTCHA\n",
    "    \n",
    "#     backbone = ResNet18Backbone(in_ch=1)\n",
    "#     x = dummy\n",
    "    \n",
    "#     print(\"Input:\", x.shape)\n",
    "    \n",
    "#     x = backbone.stem(x)\n",
    "#     print(\"After Stem:\", x.shape)\n",
    "    \n",
    "#     x = backbone.layer1(x)\n",
    "#     print(\"After Layer1:\", x.shape)\n",
    "    \n",
    "#     x = backbone.layer2(x)\n",
    "#     print(\"After Layer2:\", x.shape)\n",
    "    \n",
    "#     x = backbone.layer3(x)\n",
    "#     print(\"After Layer3:\", x.shape)\n",
    "    \n",
    "#     x = backbone.layer4(x)\n",
    "#     print(\"After Layer4:\", x.shape)\n",
    "\n",
    "\n",
    "class YOLOv8Head(nn.Module):\n",
    "    \"\"\"\n",
    "    YOLOv8-style detection head for CAPTCHA character detection\n",
    "    \n",
    "    Input: Feature maps from ResNet18 backbone (batch_size, 256, H, W)\n",
    "    Output: Predictions (batch_size, height*width*(5 + num_classes))\n",
    "           Format: [x, y, w, h, objectness, class_0, class_1, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=256, num_classes=37, height=10, width=40):\n",
    "        super(YOLOv8Head, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Output channels: bbox(4) + objectness(1) + classes(num_classes)\n",
    "        self.output_channels = 5 + num_classes\n",
    "        \n",
    "        # Feature processing layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, 256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Final prediction layer\n",
    "        self.pred_conv = nn.Conv2d(64, self.output_channels, kernel_size=1)\n",
    "        \n",
    "        # Adaptive pooling to ensure (height × width) output\n",
    "        # self.adaptive_pool = nn.AdaptiveAvgPool2d((height, width))\n",
    "        # remove adaptive pool to consitent with resnet\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Kaiming initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Feature maps from backbone (batch_size, 256, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            predictions: (batch_size, height*width*(5 + num_classes))\n",
    "        \"\"\"\n",
    "        # Process features through conv layers\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  \n",
    "        x = F.relu(self.bn2(self.conv2(x)))  \n",
    "        x = F.relu(self.bn3(self.conv3(x)))  \n",
    "        \n",
    "        # Get predictions\n",
    "        x = self.pred_conv(x)  # (batch_size, 5+num_classes, H, W)\n",
    "        \n",
    "        # Ensure fixed grid size\n",
    "        # x = self.adaptive_pool(x)  # (batch_size, 5+num_classes, height, width)\n",
    "        # remove adapative pool to make flexible and compatible with resnet\n",
    "        \n",
    "        # Reshape for loss function\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0, 2, 3, 1)  # (batch_size, height, width, 5+num_classes)\n",
    "        #x = x.view(batch_size, -1)  # (batch_size, height*width*(5+num_classes)) ---> changed to reshape\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ResNet18YOLO(nn.Module):\n",
    "    \"\"\"\n",
    "    Full CAPTCHA solver backbone + YOLO head\n",
    "    Hybrid approach: ResNet18-style backbone with dilations, YOLOv8-inspired detection head.\n",
    "    \n",
    "    Input:  (batch_size, 1, 160, 640)  grayscale CAPTCHA image\n",
    "    Output: (batch_size, 10*10*(5 + num_classes))  detection grid\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_height, grid_width, num_classes=37):\n",
    "        super(ResNet18YOLO, self).__init__()\n",
    "        # backbone\n",
    "        self.backbone = ResNet18Backbone(in_ch=1, return_p3=True)\n",
    "        # head\n",
    "        self.head = YOLOv8Head(in_channels=256, num_classes=num_classes, height=grid_height, width=grid_width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract backbone features\n",
    "        feats = self.backbone(x)              # (batch, 256, 40, 160)\n",
    "        # YOLO-style detection\n",
    "        preds = self.head(feats)              # (batch, 7*7*(5+num_classes))\n",
    "        return preds\n",
    "\n",
    "def decode_yolo_output(preds, num_classes=37, height=10, width=40, img_h=160, img_w=640, conf_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Decode YOLOv8Head predictions into bounding boxes and class labels.\n",
    "    \n",
    "    Args:\n",
    "        preds: (batch_size, height*width*(5+num_classes)) flattened tensor\n",
    "        num_classes: number of character classes\n",
    "        height, width: grid dimensions (10x40 for your case)\n",
    "        img_h, img_w: input image dimensions (160x640)\n",
    "        conf_thresh: confidence threshold for objectness\n",
    "        \n",
    "    Returns:\n",
    "        List[ List[dict] ] where each inner list is detections for one image:\n",
    "        {\n",
    "            \"x_center\": float,\n",
    "            \"y_center\": float,\n",
    "            \"width\": float,\n",
    "            \"height\": float,\n",
    "            \"class\": int,\n",
    "            \"confidence\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    batch_size = preds.shape[0]\n",
    "    cell_h = img_h / height\n",
    "    cell_w = img_w / width\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        img_preds = preds[b].view(height, width, 5 + num_classes)  # (H, W, 5+num_classes)\n",
    "        \n",
    "        boxes = []\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                tx, ty, tw, th, obj_conf, *class_logits = img_preds[i, j]\n",
    "                \n",
    "                # Apply activations\n",
    "                obj_conf = torch.sigmoid(obj_conf).item()\n",
    "                class_probs = F.softmax(torch.tensor(class_logits), dim=0)\n",
    "                cls_conf, cls_id = torch.max(class_probs, dim=0)\n",
    "                \n",
    "                # Skip low-confidence predictions\n",
    "                if obj_conf * cls_conf.item() < conf_thresh:\n",
    "                    continue\n",
    "                \n",
    "                # Decode box relative to grid cell\n",
    "                x_center = (j + torch.sigmoid(tx).item()) * cell_w\n",
    "                y_center = (i + torch.sigmoid(ty).item()) * cell_h\n",
    "                width_box = torch.exp(tw).item() * cell_w\n",
    "                height_box = torch.exp(th).item() * cell_h\n",
    "                \n",
    "                boxes.append({\n",
    "                    \"x_center\": x_center,\n",
    "                    \"y_center\": y_center,\n",
    "                    \"width\": width_box,\n",
    "                    \"height\": height_box,\n",
    "                    \"class\": int(cls_id.item()),\n",
    "                    \"confidence\": float(obj_conf * cls_conf.item())\n",
    "                })\n",
    "        \n",
    "        detections.append(boxes)\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    model = ResNet18YOLO(num_classes=37, grid_height=10, grid_width=40)\n",
    "    dummy = torch.randn(2, 1, 160, 640)  # batch=2, grayscale CAPTCHA\n",
    "    out = model(dummy)\n",
    "    print(\"Output shape:\", out.shape)   # Expected: (2, 7*7*(5+37))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
