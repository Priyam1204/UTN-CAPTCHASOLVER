{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3bc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class CaptchaDatasetWithBBoxes(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Direct path to the specific data folder (train, val, or test)\n",
    "            transform (callable, optional): Optional transform to be applied on images\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.images_dir = os.path.join(self.data_dir, 'images')\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_list = sorted([f for f in os.listdir(self.images_dir) if f.endswith('.png')])\n",
    "        \n",
    "        # Load labels if available\n",
    "        self.labels_dict = {}\n",
    "        labels_file = os.path.join(self.data_dir, 'labels.json')\n",
    "        if os.path.exists(labels_file):\n",
    "            with open(labels_file, 'r') as f:\n",
    "                labels = json.load(f)\n",
    "                self.labels_dict = {item['image_id']: item for item in labels}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        \n",
    "        # Load image as grayscale\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        original_size = image.size  # (width, height)\n",
    "        \n",
    "        # Get image_id without extension\n",
    "        image_id = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        # Get labels if available\n",
    "        label_info = self.labels_dict.get(image_id, {})\n",
    "        \n",
    "        # Extract captcha string and annotations\n",
    "        captcha_string = label_info.get('captcha_string', '')\n",
    "        annotations = label_info.get('annotations', [])\n",
    "        \n",
    "        # Process bounding boxes\n",
    "        bboxes = []\n",
    "        oriented_bboxes = []\n",
    "        category_ids = []\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            # Regular bounding box [x1, y1, x2, y2]\n",
    "            bbox = annotation.get('bbox', [])\n",
    "            if bbox:\n",
    "                bboxes.append(bbox)\n",
    "            \n",
    "            # Oriented bounding box [x1,y1,x2,y2,x3,y3,x4,y4]\n",
    "            oriented_bbox = annotation.get('oriented_bbox', [])\n",
    "            if oriented_bbox:\n",
    "                oriented_bboxes.append(oriented_bbox)\n",
    "            \n",
    "            # Category ID (character class)\n",
    "            category_id = annotation.get('category_id', -1)\n",
    "            category_ids.append(category_id)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32) if bboxes else torch.empty((0, 4))\n",
    "        oriented_bboxes = torch.tensor(oriented_bboxes, dtype=torch.float32) if oriented_bboxes else torch.empty((0, 8))\n",
    "        category_ids = torch.tensor(category_ids, dtype=torch.long) if category_ids else torch.empty((0,), dtype=torch.long)\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'image_id': image_id,\n",
    "            'captcha_string': captcha_string,\n",
    "            'bboxes': bboxes,  # Regular bounding boxes\n",
    "            'oriented_bboxes': oriented_bboxes,  # Oriented bounding boxes\n",
    "            'category_ids': category_ids,  # Character category IDs\n",
    "            'num_objects': len(annotations),  # Number of characters\n",
    "            'original_size': original_size  # (width, height)\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# Custom collate function for bounding boxes\n",
    "def collate_fn_with_bboxes(batch):\n",
    "    \"\"\"Custom collate function to handle variable-length bounding boxes\"\"\"\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    image_ids = [item['image_id'] for item in batch]\n",
    "    captcha_strings = [item['captcha_string'] for item in batch]\n",
    "    \n",
    "    # Keep bounding boxes as lists since they have variable lengths\n",
    "    bboxes = [item['bboxes'] for item in batch]\n",
    "    oriented_bboxes = [item['oriented_bboxes'] for item in batch]\n",
    "    category_ids = [item['category_ids'] for item in batch]\n",
    "    num_objects = [item['num_objects'] for item in batch]\n",
    "    original_sizes = [item['original_size'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'image_id': image_ids,\n",
    "        'captcha_string': captcha_strings,\n",
    "        'bboxes': bboxes,\n",
    "        'oriented_bboxes': oriented_bboxes,\n",
    "        'category_ids': category_ids,\n",
    "        'num_objects': num_objects,\n",
    "        'original_size': original_sizes\n",
    "    }\n",
    "\n",
    "# Helper function to create dataloaders with bboxes\n",
    "def get_dataloader_with_bboxes(data_folder, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create dataloader that includes bounding box information\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = CaptchaDatasetWithBBoxes(\n",
    "        data_dir=data_folder,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create dataloader with custom collate function\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn_with_bboxes\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Visualization function with bounding boxes\n",
    "def visualize_with_bboxes(batch, idx=0):\n",
    "    \"\"\"Visualize image with bounding boxes\"\"\"\n",
    "    # Get image and denormalize\n",
    "    img = batch['image'][idx].squeeze()\n",
    "    img = img * 0.5 + 0.5  # Denormalize from [-1, 1] to [0, 1]\n",
    "    \n",
    "    # Get bounding boxes for this image\n",
    "    bboxes = batch['bboxes'][idx]\n",
    "    category_ids = batch['category_ids'][idx]\n",
    "    captcha_string = batch['captcha_string'][idx]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f\"CAPTCHA: {captcha_string}\")\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = plt.Rectangle((x1, y1), width, height, \n",
    "                           linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add category ID as text\n",
    "        if i < len(category_ids):\n",
    "            ax.text(x1, y1-5, f'ID: {category_ids[i].item()}', \n",
    "                   color='red', fontsize=10, weight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Number of characters: {len(bboxes)}\")\n",
    "    print(f\"Bounding boxes shape: {bboxes.shape}\")\n",
    "    print(f\"Category IDs: {category_ids.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3bbec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Image shape: torch.Size([4, 1, 160, 640])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 6\n",
      "Bounding boxes shape: torch.Size([6, 4])\n",
      "Category IDs: [1, 1, 31, 23, 24, 7]\n",
      "Image 0: '11VNO7' has 6 characters\n",
      "  Bboxes shape: torch.Size([6, 4])\n",
      "Image 1: '3P' has 2 characters\n",
      "  Bboxes shape: torch.Size([2, 4])\n",
      "Image 2: 'GOPQ' has 4 characters\n",
      "  Bboxes shape: torch.Size([4, 4])\n",
      "Image 3: 'S901' has 4 characters\n",
      "  Bboxes shape: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Base path to the dataset\n",
    "base_path = '/home/utn/abap44us/Downloads/UTN-CV25-Captcha-Dataset/part2'\n",
    "\n",
    "# Create dataloaders with bounding boxes\n",
    "train_loader_bbox = get_dataloader_with_bboxes(os.path.join(base_path, 'train'), batch_size=4, shuffle=True)\n",
    "\n",
    "# Test the dataloader\n",
    "for batch in train_loader_bbox:\n",
    "    print(f\"Batch size: {len(batch['captcha_string'])}\")\n",
    "    print(f\"Image shape: {batch['image'].shape}\")\n",
    "    \n",
    "    # Visualize first image with bounding boxes\n",
    "    visualize_with_bboxes(batch, idx=0)\n",
    "    \n",
    "    # Print info about bounding boxes\n",
    "    for i in range(len(batch['captcha_string'])):\n",
    "        print(f\"Image {i}: '{batch['captcha_string'][i]}' has {batch['num_objects'][i]} characters\")\n",
    "        print(f\"  Bboxes shape: {batch['bboxes'][i].shape}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d2ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 16800])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Weight Inititializeion with He-Method ) ---\n",
    "def init_weights_kaiming(m):\n",
    "    # use Kaiming init for conv/linear layers, set bias to zero\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    # batch/group norm: start with weight=1 and bias=0\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#  Stem \n",
    "class Stem(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a small-image friendly stem that mimics the res-net18 style stem.\n",
    "\n",
    "    Tradition ResNet stem:\n",
    "        # self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # self.bn   = nn.BatchNorm2d(out_ch)\n",
    "        # self.relu = nn.ReLU(inplace=True)\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    Compared to traditional. resnet 18 this  replace 7x7 s2 + maxpool with 3×(3x3) conv stack and delay the first downsample.\n",
    "    Goal: retain fineer details early in network\n",
    "    \"\"\"\n",
    "    # def __init__(self, in_ch=3, out_ch=64):  # RGB input channels\n",
    "    def __init__(self, in_ch=1, out_ch=64):    # Grayscale Input Channels\n",
    "        super().__init__()\n",
    "\n",
    "        # --- New small-image stem ---\n",
    "        self.conv1 = nn.Conv2d(in_ch, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.act1  = nn.SiLU(inplace=True)  # see https://arxiv.org/pdf/1710.05941\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.act2  = nn.SiLU(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, out_ch, kernel_size=3, stride=2, padding=1, bias=False)  # first downsample here\n",
    "        self.bn3   = nn.BatchNorm2d(out_ch)\n",
    "        self.act3  = nn.SiLU(inplace=True)\n",
    "\n",
    "        # Initialize weights of conv layers via He_Init Strategy\n",
    "        for m in [self.conv1, self.conv2, self.conv3]:\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        for b in [self.bn1, self.bn2, self.bn3]:\n",
    "            nn.init.constant_(b.weight, 1.0)\n",
    "            nn.init.constant_(b.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "        return x  # 160x640 -> 80x320 (H×W)\n",
    "\n",
    "#  Residual Block used for resnet\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_ch, out_ch, stride=1, dilation=1 , downsample=None):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        [IMPROVEMENT] allow dilation to grow RF without more downsamples\n",
    "        [IMPROVEMENT] SiLU activations for smoother gradients on OCR edges\n",
    "        \"\"\"\n",
    "        padding = dilation\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride,\n",
    "                               padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.act   = nn.SiLU(inplace=True)  # was ReLU\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1,\n",
    "                               padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.downsample = downsample   # <--- store it\n",
    "\n",
    "\n",
    "        # Init\n",
    "        for m in (self.conv1, self.conv2):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.bn1.weight, 1.0); nn.init.constant_(self.bn1.bias, 0.0)\n",
    "        nn.init.constant_(self.bn2.weight, 1.0); nn.init.constant_(self.bn2.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.downsample is not None:   # <--- apply projection if needed\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "# ---------- ResNet-18 Backbone ----------\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Backbone only (no GAP/FC).\n",
    "\n",
    "    Old vanilla scales (for 640x160 W×H) were deeper OS; we keep higher spatial detail:\n",
    "      stem  (s2) -> 80x320\n",
    "      layer1 (s1) -> 80x320\n",
    "      layer2 (s2) -> 40x160\n",
    "      layer3 (s1, dil=2) -> 40x160\n",
    "      layer4 (s1, dil=2) -> 40x160   (256 ch)  <-- feed to your head(s)\n",
    "\n",
    "    Goal: better localization of small characters while keeping receptive field via dilation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, return_p3=True):\n",
    "        super().__init__()\n",
    "        self.stem = Stem(in_ch=in_ch, out_ch=64)\n",
    "        self.in_ch = 64\n",
    "        self.return_p3 = return_p3\n",
    "\n",
    "        # layer1: stride=1\n",
    "        self.layer1 = self._make_layer(ResidualBlock, 64,  blocks=2, stride=1, dilation=1)\n",
    "\n",
    "        # layer2: stride=2 (downsample to ~1/4 overall)\n",
    "        self.layer2 = self._make_layer(ResidualBlock, 128, blocks=2, stride=2, dilation=1)\n",
    "\n",
    "        # layer3: keep resolution (stride=1) but expand RF with dilation\n",
    "        self.layer3 = self._make_layer(ResidualBlock, 256, blocks=2, stride=1, dilation=2)\n",
    "\n",
    "        # layer4: same resolution (stride=1) with dilation\n",
    "        self.layer4 = self._make_layer(ResidualBlock, 256, blocks=2, stride=1, dilation=2)  # slimmer tail (256)\n",
    "\n",
    "    def _make_layer(self, block, out_ch, blocks, stride, dilation):\n",
    "        down = None\n",
    "        in_ch = self.in_ch\n",
    "    \n",
    "        # Projection when spatial size or channels change\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "    \n",
    "        layers = [block(in_ch, out_ch, stride=stride, dilation=dilation, downsample=down)]\n",
    "        self.in_ch = out_ch\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_ch, out_ch, stride=1, dilation=dilation, downsample=None))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)         # -> 80x320\n",
    "        x = self.layer1(x)      # -> 80x320\n",
    "        x = self.layer2(x)     # -> 40x160\n",
    "        x = self.layer3(x)     # -> 40x160\n",
    "        x = self.layer4(x)     # -> 40x160\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dummy = torch.randn(1, 1, 160, 640)  # grayscale CAPTCHA\n",
    "    \n",
    "#     backbone = ResNet18Backbone(in_ch=1)\n",
    "#     x = dummy\n",
    "    \n",
    "#     print(\"Input:\", x.shape)\n",
    "    \n",
    "#     x = backbone.stem(x)\n",
    "#     print(\"After Stem:\", x.shape)\n",
    "    \n",
    "#     x = backbone.layer1(x)\n",
    "#     print(\"After Layer1:\", x.shape)\n",
    "    \n",
    "#     x = backbone.layer2(x)\n",
    "#     print(\"After Layer2:\", x.shape)\n",
    "    \n",
    "#     x = backbone.layer3(x)\n",
    "#     print(\"After Layer3:\", x.shape)\n",
    "    \n",
    "#     x = backbone.layer4(x)\n",
    "#     print(\"After Layer4:\", x.shape)\n",
    "\n",
    "\n",
    "class YOLOv8Head(nn.Module):\n",
    "    \"\"\"\n",
    "    YOLOv8-style detection head for CAPTCHA character detection\n",
    "    \n",
    "    Input: Feature maps from ResNet18 backbone (batch_size, 256, H, W)\n",
    "    Output: Predictions (batch_size, height*width*(5 + num_classes))\n",
    "           Format: [x, y, w, h, objectness, class_0, class_1, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=256, num_classes=37, height=10, width=40):\n",
    "        super(YOLOv8Head, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "      \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Output channels: bbox(4) + objectness(1) + classes(num_classes)\n",
    "        self.output_channels = 5 + num_classes\n",
    "        \n",
    "        # Feature processing layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, 256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Final prediction layer\n",
    "        self.pred_conv = nn.Conv2d(64, self.output_channels, kernel_size=1)\n",
    "        \n",
    "        # Adaptive pooling to ensure (height × width) output\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((height, width))\n",
    "        # remove adaptive pool to consitent with resnet\n",
    "      \n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Kaiming initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Feature maps from backbone (batch_size, 256, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            predictions: (batch_size, height*width*(5 + num_classes))\n",
    "        \"\"\"\n",
    "        # Process features through conv layers\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  \n",
    "        x = F.relu(self.bn2(self.conv2(x)))  \n",
    "        x = F.relu(self.bn3(self.conv3(x)))  \n",
    "        \n",
    "        # Get predictions\n",
    "        x = self.pred_conv(x)  # (batch_size, 5+num_classes, H, W)\n",
    "        \n",
    "        # Ensure fixed grid size\n",
    "        x = self.adaptive_pool(x)  # (batch_size, 5+num_classes, height, width)\n",
    "        # remove adapative pool to make flexible and compatible with resnet\n",
    "        \n",
    "        # Reshape for loss function\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0, 2, 3, 1)  # (batch_size, height, width, 5+num_classes)\n",
    "        #x = x.view(batch_size, -1)  # (batch_size, height*width*(5+num_classes)) ---> changed to reshape\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ResNet18YOLO(nn.Module):\n",
    "    \"\"\"\n",
    "    Full CAPTCHA solver backbone + YOLO head\n",
    "    Hybrid approach: ResNet18-style backbone with dilations, YOLOv8-inspired detection head.\n",
    "    \n",
    "    Input:  (batch_size, 1, 160, 640)  grayscale CAPTCHA image\n",
    "    Output: (batch_size, 10*10*(5 + num_classes))  detection grid\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_height, grid_width, num_classes=37):\n",
    "        super(ResNet18YOLO, self).__init__()\n",
    "        # backbone\n",
    "        self.backbone = ResNet18Backbone(in_ch=1, return_p3=True)\n",
    "        # head\n",
    "        self.head = YOLOv8Head(in_channels=256, num_classes=num_classes, height=grid_height, width=grid_width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract backbone features\n",
    "        feats = self.backbone(x)              # (batch, 256, 40, 160)\n",
    "        # YOLO-style detection\n",
    "        preds = self.head(feats)              # (batch, 7*7*(5+num_classes))\n",
    "        return preds\n",
    "\n",
    "def decode_yolo_output(preds, num_classes=37, height=10, width=40, img_h=160, img_w=640, conf_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Decode YOLOv8Head predictions into bounding boxes and class labels.\n",
    "    \n",
    "    Args:\n",
    "        preds: (batch_size, height*width*(5+num_classes)) flattened tensor\n",
    "        num_classes: number of character classes\n",
    "        height, width: grid dimensions (10x40 for your case)\n",
    "        img_h, img_w: input image dimensions (160x640)\n",
    "        conf_thresh: confidence threshold for objectness\n",
    "        \n",
    "    Returns:\n",
    "        List[ List[dict] ] where each inner list is detections for one image:\n",
    "        {\n",
    "            \"x_center\": float,\n",
    "            \"y_center\": float,\n",
    "            \"width\": float,\n",
    "            \"height\": float,\n",
    "            \"class\": int,\n",
    "            \"confidence\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    batch_size = preds.shape[0]\n",
    "    cell_h = img_h / height\n",
    "    cell_w = img_w / width\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        img_preds = preds[b].view(height, width, 5 + num_classes)  # (H, W, 5+num_classes)\n",
    "        \n",
    "        boxes = []\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                tx, ty, tw, th, obj_conf, *class_logits = img_preds[i, j]\n",
    "                \n",
    "                # Apply activations\n",
    "                obj_conf = torch.sigmoid(obj_conf).item()\n",
    "                class_probs = F.softmax(torch.tensor(class_logits), dim=0)\n",
    "                cls_conf, cls_id = torch.max(class_probs, dim=0)\n",
    "                \n",
    "                # Skip low-confidence predictions\n",
    "                if obj_conf * cls_conf.item() < conf_thresh:\n",
    "                    continue\n",
    "                \n",
    "                # Decode box relative to grid cell\n",
    "                x_center = (j + torch.sigmoid(tx).item()) * cell_w\n",
    "                y_center = (i + torch.sigmoid(ty).item()) * cell_h\n",
    "                width_box = torch.exp(tw).item() * cell_w\n",
    "                height_box = torch.exp(th).item() * cell_h\n",
    "                \n",
    "                boxes.append({\n",
    "                    \"x_center\": x_center,\n",
    "                    \"y_center\": y_center,\n",
    "                    \"width\": width_box,\n",
    "                    \"height\": height_box,\n",
    "                    \"class\": int(cls_id.item()),\n",
    "                    \"confidence\": float(obj_conf * cls_conf.item())\n",
    "                })\n",
    "        \n",
    "        detections.append(boxes)\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    model = ResNet18YOLO(num_classes=37, grid_height=10, grid_width=40)\n",
    "    dummy = torch.randn(2, 1, 160, 640)  # batch=2, grayscale CAPTCHA\n",
    "    out = model(dummy)\n",
    "    print(\"Output shape:\", out.shape)   # Expected: (2, 7*7*(5+37))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e8bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_over_union(GT_bbox, PD_bbox):\n",
    "    gt_x = max(GT_bbox[0], PD_bbox[0])\n",
    "    gt_y = max(GT_bbox[1], PD_bbox[1])\n",
    "    pd_x = min(GT_bbox[2], PD_bbox[2])\n",
    "    pd_y = min(GT_bbox[3], PD_bbox[3])\n",
    "\n",
    "    # intersection area\n",
    "    inter_w = max(0, pd_x - gt_x)\n",
    "    inter_h = max(0, pd_y - gt_y)\n",
    "    intersection_area = inter_w * inter_h\n",
    "\n",
    "    # areas of the boxes\n",
    "    GT_BoxArea = max(0, GT_bbox[2] - GT_bbox[0]) * max(0, GT_bbox[3] - GT_bbox[1])\n",
    "    PD_BoxArea = max(0, PD_bbox[2] - PD_bbox[0]) * max(0, PD_bbox[3] - PD_bbox[1])\n",
    "\n",
    "    # IoU\n",
    "    union = GT_BoxArea + PD_BoxArea - intersection_area\n",
    "    intersection_over_union = intersection_area / float(union + 1e-9)\n",
    "    return intersection_over_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b49ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_over_union(GT_bbox, PD_bbox):\n",
    "    gt_x = max(GT_bbox[0], PD_bbox[0])\n",
    "    gt_y = max(GT_bbox[1], PD_bbox[1])\n",
    "    pd_x = min(GT_bbox[2], PD_bbox[2])\n",
    "    pd_y = min(GT_bbox[3], PD_bbox[3])\n",
    "\n",
    "    # intersection area\n",
    "    inter_w = max(0, pd_x - gt_x)\n",
    "    inter_h = max(0, pd_y - gt_y)\n",
    "    intersection_area = inter_w * inter_h\n",
    "\n",
    "    # areas of the boxes\n",
    "    GT_BoxArea = max(0, GT_bbox[2] - GT_bbox[0]) * max(0, GT_bbox[3] - GT_bbox[1])\n",
    "    PD_BoxArea = max(0, PD_bbox[2] - PD_bbox[0]) * max(0, PD_bbox[3] - PD_bbox[1])\n",
    "\n",
    "    # IoU\n",
    "    union = GT_BoxArea + PD_BoxArea - intersection_area\n",
    "    intersection_over_union = intersection_area / float(union + 1e-9)\n",
    "    return intersection_over_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc69dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        box1: [x1, y1, x2, y2] - Bounding box 1\n",
    "        box2: [x1, y1, x2, y2] - Bounding box 2\n",
    "    \n",
    "    Returns:\n",
    "        IoU: Intersection over Union value\n",
    "    \"\"\"\n",
    "    # Calculate intersection\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "    # Calculate union\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if union_area == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def apply_nms(detections, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression (NMS) to filter overlapping bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        detections: List of detections, each detection is a dictionary:\n",
    "                    {'bbox': [x1, y1, x2, y2], 'confidence': float, 'class_id': int}\n",
    "        iou_threshold: IoU threshold for suppressing overlapping boxes.\n",
    "    \n",
    "    Returns:\n",
    "        List of filtered detections after NMS.\n",
    "    \"\"\"\n",
    "    # Sort detections by confidence score in descending order\n",
    "    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
    "    filtered_detections = []\n",
    "\n",
    "    while detections:\n",
    "        # Pick the detection with the highest confidence\n",
    "        best = detections.pop(0)\n",
    "        filtered_detections.append(best)\n",
    "\n",
    "        # Remove detections with IoU > threshold\n",
    "        detections = [\n",
    "            det for det in detections\n",
    "            if calculate_iou(best['bbox'], det['bbox']) < iou_threshold\n",
    "        ]\n",
    "\n",
    "    return filtered_detections\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e706d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e652ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv8Loss(nn.Module):\n",
    "    \"\"\"YOLO-style loss over an H×W grid.\"\"\"\n",
    "    def __init__(self, num_classes=37, H=40, W=160, \n",
    "                 lambda_coord=5.0, lambda_obj=1.0, lambda_class=1.0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_obj = lambda_obj\n",
    "        self.lambda_class = lambda_class\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: (B, H*W*(5+C)) flattened\n",
    "        targets:     (B, H, W, 5+C)\n",
    "        \"\"\"\n",
    "        B = predictions.size(0)\n",
    "        # reshape to match targets\n",
    "        predictions = predictions.view(B, self.H, self.W, 5 + self.num_classes)\n",
    "\n",
    "        # split\n",
    "        pred_bbox  = predictions[..., :4]\n",
    "        pred_obj   = predictions[..., 4:5]\n",
    "        pred_class = predictions[..., 5:]\n",
    "\n",
    "        target_bbox  = targets[..., :4]\n",
    "        target_obj   = targets[..., 4:5]\n",
    "        target_class = targets[..., 5:]\n",
    "\n",
    "        obj_mask   = (target_obj > 0)\n",
    "        noobj_mask = ~obj_mask\n",
    "\n",
    "        # 1) bbox loss (only positive)\n",
    "        bbox_loss = 0.0\n",
    "        if obj_mask.any():\n",
    "            bbox_loss = F.smooth_l1_loss(\n",
    "                pred_bbox[obj_mask.expand_as(pred_bbox)],\n",
    "                target_bbox[obj_mask.expand_as(target_bbox)],\n",
    "                reduction='sum'\n",
    "            )\n",
    "\n",
    "        # 2) objectness loss\n",
    "        obj_loss = 0.0\n",
    "        if obj_mask.any():\n",
    "            obj_loss = F.binary_cross_entropy_with_logits(\n",
    "                pred_obj[obj_mask], target_obj[obj_mask], reduction='sum'\n",
    "            )\n",
    "        noobj_loss = 0.0\n",
    "        if noobj_mask.any():\n",
    "            noobj_loss = F.binary_cross_entropy_with_logits(\n",
    "                pred_obj[noobj_mask], torch.zeros_like(pred_obj[noobj_mask]), reduction='sum'\n",
    "            )\n",
    "        total_obj_loss = obj_loss + noobj_loss\n",
    "\n",
    "        # 3) class loss (only positive)\n",
    "        class_loss = 0.0\n",
    "        if obj_mask.any():\n",
    "            target_class_indices = torch.argmax(target_class, dim=-1)   # (B,H,W)\n",
    "            obj_mask_flat = obj_mask.squeeze(-1)                        # (B,H,W)\n",
    "            pred_class_pos = pred_class[obj_mask_flat]                  # (Npos, C)\n",
    "            target_class_pos = target_class_indices[obj_mask_flat]      # (Npos,)\n",
    "            if pred_class_pos.numel() > 0:\n",
    "                class_loss = F.cross_entropy(pred_class_pos, target_class_pos, reduction='sum')\n",
    "\n",
    "        total = (self.lambda_coord * bbox_loss +\n",
    "                 self.lambda_obj   * total_obj_loss +\n",
    "                 self.lambda_class * class_loss) / B\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": total,\n",
    "            \"bbox_loss\": bbox_loss / B,\n",
    "            \"obj_loss\": total_obj_loss / B,\n",
    "            \"class_loss\": class_loss / B,\n",
    "            \"num_pos\": int(obj_mask.sum().item()),\n",
    "            \"num_neg\": int(noobj_mask.sum().item())\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ffe06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetPreparer:\n",
    "    \"\"\"Build (B, H, W, 5+C) targets from variable bbox formats.\"\"\"\n",
    "    def __init__(self, H=40, W=160, num_classes=37, img_width=640, img_height=160):\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.num_classes = num_classes\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_xyxy(b):\n",
    "        if isinstance(b, dict) and 'bbox' in b and len(b['bbox']) == 4:\n",
    "            x, y, w, h = b['bbox']\n",
    "            return float(x), float(y), float(x+w), float(y+h)\n",
    "        if torch.is_tensor(b):\n",
    "            if b.numel() == 4:\n",
    "                b = b.reshape(-1)\n",
    "                return float(b[0]), float(b[1]), float(b[2]), float(b[3])\n",
    "            if b.dim() == 2 and b.shape == (1,4):\n",
    "                b = b.squeeze(0)\n",
    "                return float(b[0]), float(b[1]), float(b[2]), float(b[3])\n",
    "            raise ValueError(f\"Unsupported bbox tensor shape: {tuple(b.shape)}\")\n",
    "        if isinstance(b, (list, tuple)):\n",
    "            if len(b) == 1 and isinstance(b[0], (list, tuple, torch.Tensor)):\n",
    "                return TargetPreparer._to_xyxy(b[0])\n",
    "            if len(b) == 4:\n",
    "                return float(b[0]), float(b[1]), float(b[2]), float(b[3])\n",
    "            raise ValueError(f\"Unsupported bbox list/tuple: {b}\")\n",
    "        raise ValueError(f\"Unsupported bbox type: {type(b)}\")\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        B = len(batch['captcha_string'])\n",
    "        targets = torch.zeros(B, self.H, self.W, 5 + self.num_classes)\n",
    "\n",
    "        for b in range(B):\n",
    "            bboxes = batch['bboxes'][b]\n",
    "            class_ids = batch['category_ids'][b]\n",
    "\n",
    "            if bboxes is None or (torch.is_tensor(bboxes) and bboxes.numel()==0) or (isinstance(bboxes,(list,tuple)) and len(bboxes)==0):\n",
    "                continue\n",
    "\n",
    "            if torch.is_tensor(bboxes) and bboxes.dim()==2 and bboxes.shape[1]==4:\n",
    "                N = bboxes.shape[0]\n",
    "                boxes_iter = [bboxes[i] for i in range(N)]\n",
    "            else:\n",
    "                boxes_iter = list(bboxes)\n",
    "\n",
    "            if torch.is_tensor(class_ids):\n",
    "                if class_ids.dim()==0:\n",
    "                    class_ids = [int(class_ids.item())]*len(boxes_iter)\n",
    "                else:\n",
    "                    class_ids = [int(c) for c in class_ids.tolist()]\n",
    "            else:\n",
    "                class_ids = [int(c) for c in list(class_ids)]\n",
    "\n",
    "            assert len(boxes_iter)==len(class_ids), \"bboxes and class_ids length mismatch\"\n",
    "\n",
    "            for bb, cid in zip(boxes_iter, class_ids):\n",
    "                x1,y1,x2,y2 = self._to_xyxy(bb)\n",
    "\n",
    "                cx = ((x1+x2)/2.0) / self.img_width\n",
    "                cy = ((y1+y2)/2.0) / self.img_height\n",
    "                w  = (x2-x1) / self.img_width\n",
    "                h  = (y2-y1) / self.img_height\n",
    "\n",
    "                gx = min(max(int(cx * self.W), 0), self.W-1)\n",
    "                gy = min(max(int(cy * self.H), 0), self.H-1)\n",
    "\n",
    "                rx = cx * self.W - gx\n",
    "                ry = cy * self.H - gy\n",
    "\n",
    "                targets[b, gy, gx, 0] = rx\n",
    "                targets[b, gy, gx, 1] = ry\n",
    "                targets[b, gy, gx, 2] = w\n",
    "                targets[b, gy, gx, 3] = h\n",
    "                targets[b, gy, gx, 4] = 1.0\n",
    "                if 0 <= cid < self.num_classes:\n",
    "                    targets[b, gy, gx, 5 + cid] = 1.0\n",
    "\n",
    "        return targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "556c297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOTrainer:\n",
    "    \"\"\"Simple training wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone, yolo_head, loss_fn, target_preparer, device='cuda'):\n",
    "        self.backbone = backbone\n",
    "        self.yolo_head = yolo_head\n",
    "        self.loss_fn = loss_fn\n",
    "        self.target_preparer = target_preparer\n",
    "        self.device = device\n",
    "        \n",
    "        self.backbone.to(device)\n",
    "        self.yolo_head.to(device)\n",
    "        self.loss_fn.to(device)\n",
    "    \n",
    "    def train_step(self, batch, optimizer):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.backbone.train()\n",
    "        self.yolo_head.train()\n",
    "        \n",
    "        images = batch['image'].to(self.device)\n",
    "        targets = self.target_preparer(batch).to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        features = self.backbone(images)\n",
    "        predictions = self.yolo_head(features)\n",
    "        loss_dict = self.loss_fn(predictions, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict['total_loss'].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return {k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()}\n",
    "    \n",
    "    def validate_step(self, batch):\n",
    "        \"\"\"Single validation step\"\"\"\n",
    "        self.backbone.eval()\n",
    "        self.yolo_head.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            images = batch['image'].to(self.device)\n",
    "            targets = self.target_preparer(batch).to(self.device)\n",
    "            \n",
    "            features = self.backbone(images)\n",
    "            predictions = self.yolo_head(features)\n",
    "            loss_dict = self.loss_fn(predictions, targets)\n",
    "        \n",
    "        return {k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982e0b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "✅ Training setup complete!\n",
      "Ready to integrate with your ResNet-18 backbone and YOLO head\n"
     ]
    }
   ],
   "source": [
    "# Example usage with your models\n",
    "def setup_training():\n",
    "    \"\"\"Setup training components\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize loss and target preparer\n",
    "    loss_fn = YOLOv8Loss(num_classes=37, S=7, lambda_coord=5.0, lambda_obj=1.0, lambda_class=1.0)\n",
    "    target_preparer = TargetPreparer(S=7, num_classes=37, img_width=640, img_height=160)\n",
    "    \n",
    "    # Initialize trainer (uncomment when you have your models)\n",
    "    # trainer = YOLOTrainer(backbone, yolo_head, loss_fn, target_preparer, device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     list(backbone.parameters()) + list(yolo_head.parameters()), \n",
    "    #     lr=0.001\n",
    "    # )\n",
    "    \n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"✅ Training setup complete!\")\n",
    "    print(\"Ready to integrate with your ResNet-18 backbone and YOLO head\")\n",
    "    \n",
    "    return loss_fn, target_preparer\n",
    "\n",
    "loss_fn, target_preparer = setup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eeb91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, n=10, img_h=160, img_w=640):\n",
    "        self.n = n\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "\n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.randn(1, self.img_h, self.img_w)   # grayscale\n",
    "        bboxes = torch.tensor([[100, 50, 150, 100]], dtype=torch.float32)  # (N,4)\n",
    "        cats   = torch.tensor([0], dtype=torch.long)                       # (N,)\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"captcha_string\": \"A\",     # string, not ['A']\n",
    "            \"bboxes\": bboxes,          # tensor (N,4)\n",
    "            \"category_ids\": cats       # tensor (N,)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9586fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "{'total_loss': 2459.51171875, 'bbox_loss': 2.8663482666015625, 'obj_loss': 2440.417236328125, 'class_loss': 4.762690544128418, 'num_pos': 2, 'num_neg': 12798}\n",
      "{'total_loss': 2689.287841796875, 'bbox_loss': 2.4127917289733887, 'obj_loss': 2672.543212890625, 'class_loss': 4.680756568908691, 'num_pos': 2, 'num_neg': 12798}\n",
      "{'total_loss': 2293.541748046875, 'bbox_loss': 2.4065775871276855, 'obj_loss': 2275.082763671875, 'class_loss': 6.4260663986206055, 'num_pos': 2, 'num_neg': 12798}\n",
      "{'total_loss': 1867.5504150390625, 'bbox_loss': 0.6488494277000427, 'obj_loss': 1858.9373779296875, 'class_loss': 5.3687286376953125, 'num_pos': 2, 'num_neg': 12798}\n",
      "{'total_loss': 1795.52685546875, 'bbox_loss': 1.523563265800476, 'obj_loss': 1782.7413330078125, 'class_loss': 5.167764663696289, 'num_pos': 2, 'num_neg': 12798}\n",
      "\n",
      "Epoch 2\n",
      "{'total_loss': 1665.2777099609375, 'bbox_loss': 3.0979299545288086, 'obj_loss': 1646.201171875, 'class_loss': 3.58687162399292, 'num_pos': 2, 'num_neg': 12798}\n",
      "{'total_loss': 1541.4393310546875, 'bbox_loss': 2.9059054851531982, 'obj_loss': 1522.8779296875, 'class_loss': 4.031824588775635, 'num_pos': 2, 'num_neg': 12798}\n",
      "{'total_loss': 1415.6331787109375, 'bbox_loss': 0.6991665363311768, 'obj_loss': 1406.78515625, 'class_loss': 5.352164268493652, 'num_pos': 2, 'num_neg': 12798}\n",
      "{'total_loss': 1326.9853515625, 'bbox_loss': 2.2518136501312256, 'obj_loss': 1311.012939453125, 'class_loss': 4.7134013175964355, 'num_pos': 2, 'num_neg': 12798}\n",
      "{'total_loss': 1222.42822265625, 'bbox_loss': 1.6164970397949219, 'obj_loss': 1209.2479248046875, 'class_loss': 5.097725868225098, 'num_pos': 2, 'num_neg': 12798}\n",
      "Validation: {'total_loss': 1511.9691162109375, 'bbox_loss': 1.4841868877410889, 'obj_loss': 1500.93603515625, 'class_loss': 3.6121578216552734, 'num_pos': 2, 'num_neg': 12798}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "backbone = ResNet18Backbone(in_ch=1)\n",
    "head     = YOLOv8Head(in_channels=256, num_classes=37, height=40, width=160)\n",
    "\n",
    "loss_fn  = YOLOv8Loss(num_classes=37, H=40, W=160)\n",
    "targets  = TargetPreparer(H=40, W=160, num_classes=37, img_width=640, img_height=160)\n",
    "\n",
    "trainer  = YOLOTrainer(backbone, head, loss_fn, targets, device=device)\n",
    "optim    = torch.optim.Adam(list(backbone.parameters()) + list(head.parameters()), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(DummyDataset(), batch_size=2, shuffle=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    for batch in train_loader:\n",
    "        loss_dict = trainer.train_step(batch, optim)\n",
    "        print({k: float(v) if torch.is_tensor(v) else v for k,v in loss_dict.items()})\n",
    "\n",
    "# quick val\n",
    "for batch in train_loader:\n",
    "    print(\"Validation:\", trainer.validate_step(batch))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   5%|▌         | 382/7500 [18:18<6:21:06,  3.21s/it, t_loss=16.72, bbox=0.44, obj+=1.49, obj-=2.18, cls=10.86, lr=1.00e-03]   "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ---- config ----\n",
    "base_path = '/home/utn/abap44us/Downloads/UTN-CV25-Captcha-Dataset/part2'\n",
    "H, W = 40, 160             # backbone feature map size\n",
    "NUM_CLASSES = 37\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "CKPT_DIR = './checkpoints'\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- dataloaders (train/val) ----\n",
    "train_loader = get_dataloader_with_bboxes(os.path.join(base_path, 'train'),\n",
    "                                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = get_dataloader_with_bboxes(os.path.join(base_path, 'val'),\n",
    "                                          batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ---- models ----\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "backbone = ResNet18Backbone(in_ch=1)\n",
    "head     = YOLOv8Head(in_channels=256, num_classes=NUM_CLASSES, height=H, width=W)\n",
    "\n",
    "# ---- loss & targets (H/W version) ----\n",
    "# If your current loss is SxS, replace with the H/W-capable version or keep your H/W version.\n",
    "class YOLOv8LossHW(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, H=H, W=W,\n",
    "                 lambda_coord=5.0, lambda_obj=1.0, lambda_class=1.0, neg_weight=0.05):\n",
    "        super().__init__()\n",
    "        self.C = num_classes; self.H = H; self.W = W\n",
    "        self.lc = lambda_coord; self.lo = lambda_obj; self.lcl = lambda_class\n",
    "        self.neg_weight = neg_weight\n",
    "    def forward(self, predictions, targets):\n",
    "        B = predictions.size(0)\n",
    "        preds = predictions.view(B, self.H, self.W, 5 + self.C)\n",
    "        pb, po, pc = preds[..., :4], preds[..., 4:5], preds[..., 5:]\n",
    "        tb, to, tc = targets[..., :4], targets[..., 4:5], targets[..., 5:]\n",
    "\n",
    "        obj_mask   = (to > 0)\n",
    "        noobj_mask = ~obj_mask\n",
    "\n",
    "        bbox_loss = 0.0\n",
    "        if obj_mask.any():\n",
    "            bbox_loss = F.smooth_l1_loss(pb[obj_mask.expand_as(pb)],\n",
    "                                         tb[obj_mask.expand_as(tb)],\n",
    "                                         reduction='sum')\n",
    "\n",
    "        pos_obj_loss = 0.0\n",
    "        if obj_mask.any():\n",
    "            pos_obj_loss = F.binary_cross_entropy_with_logits(po[obj_mask], to[obj_mask], reduction='sum')\n",
    "\n",
    "        neg_obj_loss = 0.0\n",
    "        if noobj_mask.any():\n",
    "            neg_obj_loss = self.neg_weight * F.binary_cross_entropy_with_logits(\n",
    "                po[noobj_mask], torch.zeros_like(po[noobj_mask]), reduction='sum')\n",
    "\n",
    "        class_loss = 0.0\n",
    "        if obj_mask.any():\n",
    "            t_idx = torch.argmax(tc, dim=-1)\n",
    "            mask  = obj_mask.squeeze(-1)\n",
    "            pc_pos = pc[mask]; t_pos = t_idx[mask]\n",
    "            if pc_pos.numel() > 0:\n",
    "                class_loss = F.cross_entropy(pc_pos, t_pos, reduction='sum')\n",
    "\n",
    "        total = self.lc*bbox_loss + self.lo*(pos_obj_loss + neg_obj_loss) + self.lcl*class_loss\n",
    "\n",
    "        # normalize by batch\n",
    "        for k in ['total','bbox_loss','pos_obj_loss','neg_obj_loss','class_loss']:\n",
    "            pass\n",
    "        out = {\n",
    "            'total_loss': total / B,\n",
    "            'bbox_loss': bbox_loss / B,\n",
    "            'pos_obj_loss': pos_obj_loss / B,\n",
    "            'neg_obj_loss': neg_obj_loss / B,\n",
    "            'class_loss': class_loss / B,\n",
    "            'num_pos': obj_mask.sum().item(),\n",
    "            'num_neg': noobj_mask.sum().item()\n",
    "        }\n",
    "        return out\n",
    "\n",
    "loss_fn = YOLOv8LossHW(num_classes=NUM_CLASSES, H=H, W=W)\n",
    "\n",
    "# ---- target preparer (H/W grid) ----\n",
    "class TargetPreparerHW:\n",
    "    def __init__(self, H=H, W=W, num_classes=NUM_CLASSES, img_width=640, img_height=160):\n",
    "        self.H = H; self.W = W; self.C = num_classes\n",
    "        self.img_w = img_width; self.img_h = img_height\n",
    "    def __call__(self, batch):\n",
    "        B = len(batch['captcha_string'])\n",
    "        tgt = torch.zeros(B, self.H, self.W, 5 + self.C)\n",
    "        for b in range(B):\n",
    "            if len(batch['bboxes'][b]) == 0: continue\n",
    "            bboxes = batch['bboxes'][b]; cats = batch['category_ids'][b]\n",
    "            for i in range(len(bboxes)):\n",
    "                x1,y1,x2,y2 = bboxes[i]\n",
    "                cx = (x1+x2)/(2.0*self.img_w); cy = (y1+y2)/(2.0*self.img_h)\n",
    "                w  = (x2-x1)/self.img_w;       h  = (y2-y1)/self.img_h\n",
    "                gx = min(max(int(cx*self.W), 0), self.W-1)\n",
    "                gy = min(max(int(cy*self.H), 0), self.H-1)\n",
    "                rel_x = cx*self.W - gx; rel_y = cy*self.H - gy\n",
    "\n",
    "                tgt[b, gy, gx, 0] = rel_x\n",
    "                tgt[b, gy, gx, 1] = rel_y\n",
    "                tgt[b, gy, gx, 2] = w\n",
    "                tgt[b, gy, gx, 3] = h\n",
    "                tgt[b, gy, gx, 4] = 1.0\n",
    "                cls = int(cats[i].item())\n",
    "                if 0 <= cls < self.C:\n",
    "                    tgt[b, gy, gx, 5 + cls] = 1.0\n",
    "        return tgt\n",
    "\n",
    "target_preparer = TargetPreparerHW(H=H, W=W, num_classes=NUM_CLASSES)\n",
    "\n",
    "# ---- trainer ----\n",
    "trainer = YOLOTrainer(backbone, head, loss_fn, target_preparer, device=device)\n",
    "\n",
    "# ---- optimizer & (optional) scheduler ----\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(backbone.parameters()) + list(head.parameters()), lr=LR\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# ---- training with best checkpoint saving ----\n",
    "best_val = float('inf')\n",
    "best_path = os.path.join(CKPT_DIR, 'best.pth')\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'\\nEpoch {epoch}/{EPOCHS}')\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ---------- TRAIN ----------\n",
    "    trainer.backbone.train()\n",
    "    trainer.yolo_head.train()\n",
    "\n",
    "    train_running = {\n",
    "        'total_loss': 0.0,\n",
    "        'bbox_loss': 0.0,\n",
    "        'pos_obj_loss': 0.0,\n",
    "        'neg_obj_loss': 0.0,\n",
    "        'class_loss': 0.0,\n",
    "        'num_pos': 0,\n",
    "        'num_neg': 0\n",
    "    }\n",
    "    num_train_batches = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc='Train', leave=False)\n",
    "    for batch in pbar:\n",
    "        log = trainer.train_step(batch, optimizer)\n",
    "        num_train_batches += 1\n",
    "\n",
    "        # accumulate\n",
    "        for k in train_running:\n",
    "            if k in log:\n",
    "                train_running[k] += float(log[k])\n",
    "\n",
    "        # live progress\n",
    "        pbar.set_postfix({\n",
    "            't_loss': f\"{log['total_loss']:.2f}\",\n",
    "            'bbox': f\"{log.get('bbox_loss', 0):.2f}\",\n",
    "            'obj+': f\"{log.get('pos_obj_loss', 0):.2f}\",\n",
    "            'obj-': f\"{log.get('neg_obj_loss', 0):.2f}\",\n",
    "            'cls':  f\"{log.get('class_loss', 0):.2f}\",\n",
    "            'lr':   f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        })\n",
    "\n",
    "    # scheduler step (per-epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    # epoch train averages\n",
    "    train_avg = {k: v / max(1, num_train_batches) for k, v in train_running.items()}\n",
    "\n",
    "    # ---------- VALIDATE ----------\n",
    "    trainer.backbone.eval()\n",
    "    trainer.yolo_head.eval()\n",
    "\n",
    "    val_running = {\n",
    "        'total_loss': 0.0,\n",
    "        'bbox_loss': 0.0,\n",
    "        'pos_obj_loss': 0.0,\n",
    "        'neg_obj_loss': 0.0,\n",
    "        'class_loss': 0.0,\n",
    "        'num_pos': 0,\n",
    "        'num_neg': 0\n",
    "    }\n",
    "    num_val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Val', leave=False):\n",
    "            v = trainer.validate_step(batch)\n",
    "            num_val_batches += 1\n",
    "            for k in val_running:\n",
    "                if k in v:\n",
    "                    val_running[k] += float(v[k])\n",
    "\n",
    "    val_avg = {k: v / max(1, num_val_batches) for k, v in val_running.items()}\n",
    "\n",
    "    # ---------- EPOCH SUMMARY ----------\n",
    "    elapsed = time.time() - t0\n",
    "    print(\n",
    "        f\"Train  | total: {train_avg['total_loss']:.3f} | \"\n",
    "        f\"bbox: {train_avg['bbox_loss']:.3f} | obj+: {train_avg['pos_obj_loss']:.3f} | \"\n",
    "        f\"obj-: {train_avg['neg_obj_loss']:.3f} | cls: {train_avg['class_loss']:.3f} | \"\n",
    "        f\"pos/neg: {int(train_avg['num_pos'])}/{int(train_avg['num_neg'])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Val    | total: {val_avg['total_loss']:.3f} | \"\n",
    "        f\"bbox: {val_avg['bbox_loss']:.3f} | obj+: {val_avg['pos_obj_loss']:.3f} | \"\n",
    "        f\"obj-: {val_avg['neg_obj_loss']:.3f} | cls: {val_avg['class_loss']:.3f} | \"\n",
    "        f\"pos/neg: {int(val_avg['num_pos'])}/{int(val_avg['num_neg'])}\"\n",
    "    )\n",
    "    print(f\"Time   | {elapsed:.1f}s   LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # ---------- SAVE BEST ----------\n",
    "    if val_avg['total_loss'] < best_val:\n",
    "        best_val = val_avg['total_loss']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'backbone': backbone.state_dict(),\n",
    "            'head': head.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_val': best_val,\n",
    "            'H': H, 'W': W, 'num_classes': NUM_CLASSES\n",
    "        }, best_path)\n",
    "        print(f'✅ New best saved to {best_path}  (val={best_val:.4f})')\n",
    "\n",
    "print(\"\\nTraining done.\")\n",
    "print(f\"Best val loss: {best_val:.4f}  ->  checkpoint: {best_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
